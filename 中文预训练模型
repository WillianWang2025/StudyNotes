# OpenCLaP：多领域开源中文预训练语言模型仓库
由清华大学提供的基于transformer的预训练模型：

以下是我们目前公开发布的模型概览：

名称	基础模型	数据来源	训练数据大小	词表大小	模型大小	下载地址
民事文书BERT	bert-base	全部民事文书	2654万篇文书	22554	370MB	点我下载
刑事文书BERT	bert-base	全部刑事文书	663万篇文书	22554	370MB	点我下载
百度百科BERT	bert-base	百度百科	903万篇词条	22166	367MB	点我下载
